{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is written to extract topics from text data. The code supports performing the following analysis:-\n",
    "1. Latent Dirichlet Allocation (LDA)\n",
    "2. Supervised Latent Dirichlet Allocation (sLDA)\n",
    "\n",
    "Latent Dirichlet allocation can be performed using Gensim or Tomotopy. Supervised LDA can be performed using Tomotopy. The dependent variable can be linear or binary in nature.\n",
    "\n",
    "In addition to this, the code also allows users to evaluate models using measures such as Coherence and Perplexity. Various visualisations can also be used to evaluate the results from the topic models. These include:-\n",
    "1. pyLDAvis to understand topics and the inter-topic distance\n",
    "2. Word clouds for topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ************************** Importing Packages ************************ ###\n",
    "from __future__ import division\n",
    "import re                     # regular expressions\n",
    "import numpy as np            # scientific computing\n",
    "import pandas as pd           # datastructures and computing\n",
    "import pprint as pprint       # better printing\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "#import graphlab as gl\n",
    "import pyLDAvis               # interactive topic model visualisation\n",
    "#import pyLDAvis.graphlab\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline          # to ensure that the matplotlib plots are printed in the Jupyter notebooks\n",
    "\n",
    "# Libraries for Topic Models\n",
    "import sys\n",
    "import tomotopy as tp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the list of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['app', 'apps', 'also', 'android', 'atm', 'atms', 'call', 'calls', 'calling', 'browsing', 'browse', \n",
    "                   'contact','clock', 'communication', 'dk', 'edu', 'e-mails', 'email', 'emails', 'etc', 'etfc', \n",
    "                   'entertainment', 'fb', 'files', 'from', 'food', 'images', 'info', 'internet', 'jpg', 'online', \n",
    "                   'mail', 'make', 'much', 'mean', 'music', 'messaging', 'messenger', 'mobilepay', 'nd', 'networks', \n",
    "                   'news', 'parents', 'friends', 'pdf', 'player', 'photos', 'photographs', 'photography', 'receiving', \n",
    "                   're', 'related', 'reviews', 'rooms', 'social', 'subject', 'sms', 'storage', 'skype', 'twitter', \n",
    "                   'text', 'texting', 'use', 'using', 'www', 'wifi'])\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset preparation is quite important here. If the model used is simple LDA, only the text data is mandatory.\n",
    "\n",
    "However, for the supervised LDA, it requires the response variable (dependent variable) along with the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ************************** Importing Datasets ************************ ###\n",
    "directo = \"C:\\\\Users\\\\vibabu\\\\Dropbox\\\\Doctoral_Research\\\\STSM\\\\Analysis\\\\Combined_Dataset\\\\SEM_October_19\\\\India_OE\\\\Results_Feb_10\\\\Open_Ended_Q1\"\n",
    "\n",
    "df = pd.read_csv(directo + \"\\\\Open_Ended_Q1_90_pct.csv\", encoding='latin-1')\n",
    "print(df.Response.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the content field in dataset into a list\n",
    "data = df.Response.values.tolist()\n",
    "resp = df.Rating.values.tolist()\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code, the following data cleaning techniques are used:-\n",
    "\n",
    "1. E-mail id  and New line characters\n",
    "2. Remove \"StopWords\" from the dataset\n",
    "3. Forming bigrams and trigrams\n",
    "4. Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing emails and new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ************************** Datasets Cleaning ************************* ###\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\n', ' ', sent) for sent in data]\n",
    "\n",
    "pprint.pprint(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove StopWords\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "        \"\"\"\n",
    "        objective:\n",
    "            function to remove stopwords from the paragraph/sentence\n",
    "            uses the preprocess\n",
    "        input:\n",
    "            paragraph/sentences\n",
    "        output:\n",
    "            wordlist after the stopwords are removed\n",
    "        \"\"\"\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "             for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data)\n",
    "data_words_nostops[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forming Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    \"\"\"\n",
    "    objective:\n",
    "        takes the processed text- after preprocessing and stop word removal\n",
    "    input:\n",
    "        preprocessed text\n",
    "    output:\n",
    "        text with bigrams\n",
    "    \"\"\"\n",
    "    return [bigram_mod[text] for text in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    \"\"\"\n",
    "    objective:\n",
    "        generate trigrams for the text\n",
    "    input:\n",
    "        text with bigrams\n",
    "    output:\n",
    "        text with trigrams    \n",
    "    \"\"\"\n",
    "    return [trigram_mod[bigram_mod[text]] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Dataset\n",
    "\n",
    "# Build functions to remove stopwords, bigram and trigram models- calibration dataset\n",
    "bigram = gensim.models.phrases.Phrases(data, min_count=5, threshold=100)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[data], threshold=100)\n",
    "\n",
    "# Passing the parameters to the bigram/trigram- calibration dataset\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_bigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "data_lemmatized = []\n",
    "for texts in data_words_bigrams:\n",
    "    data_lemmatized.append([ps.stem(doc) for doc in texts])\n",
    "    \n",
    "data_lemmatized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Files to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Data'] = data_lemmatized\n",
    "df.head()\n",
    "df.to_csv(directo + \"\\\\Output_Q1_Words_Calibration.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LDA Function\n",
    "    \n",
    "def lda_model(input_list, save_path):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        the function estimates the LDA model and outputs the estimated topics\n",
    "    input:\n",
    "        list with documents as responses\n",
    "    output:\n",
    "        prints the topics\n",
    "        words and their corresponding proportions\n",
    "    \"\"\"\n",
    "    mdl = tp.LDAModel(tw=tp.TermWeight.ONE,             # Term weighting\n",
    "                      min_cf=3,                         # Minimum frequency of words\n",
    "                      rm_top=0,                         # Number of top frequency words to be removed\n",
    "                      k=3)                              # Number of topics\n",
    "    for n, line in enumerate(input_list):\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        mdl.add_doc(docu)\n",
    "    mdl.burn_in = 100\n",
    "    mdl.train(0)\n",
    "    print('Num docs: ', len(mdl.docs), 'Vocab size: ', mdl.num_vocabs, 'Num words: ', mdl.num_words)\n",
    "    print('Removed words: ', mdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        mdl.train(1000)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "        \n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    mdl.save(save_path, True)\n",
    "    \n",
    "    for k in range(mdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in mdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running LDA')\n",
    "lda_model = lda_model(data_lemmatized, 'test.lda.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slda_model(documents, dep_var, save_path):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        the function estimates the sLDA model and outputs the estimated topics\n",
    "    input:\n",
    "        list with documents as responses\n",
    "        dependent variable\n",
    "    output:\n",
    "        prints the topics\n",
    "        words and their corresponding proportions\n",
    "    \"\"\"\n",
    "    smdl = tp.SLDAModel(tw=tp.TermWeight.ONE,             # Term weighting\n",
    "                        min_cf=3,                         # Minimum frequency of words\n",
    "                        rm_top=0,                         # Number of top frequency words to be removed\n",
    "                        k=3,                              # Number of topics\n",
    "                        vars=['l'])                       # Number of dependent variables\n",
    "    for row, pred in zip(documents, dep_var):\n",
    "        pred_1 = []\n",
    "        pred_1.append(pred)\n",
    "        ch = \" \".join(row)\n",
    "        docu = ch.strip().split()\n",
    "        smdl.add_doc(words=docu, y=pred_1)\n",
    "        \n",
    "    smdl.burn_in = 100\n",
    "    smdl.train(0)\n",
    "    \n",
    "    # Printing the output statistics\n",
    "    print('Num docs: ', len(smdl.docs), 'Vocab size: ', smdl.num_vocabs, 'Num words: ', smdl.num_words)\n",
    "    print('Removed top words: ', smdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        smdl.train(1000)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, smdl.ll_per_word))\n",
    "        \n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    smdl.save(save_path, True)\n",
    "    \n",
    "    for k in range(smdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in smdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "    return smdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running Supervised LDA')\n",
    "slda_model = slda_model(data_lemmatized, resp, 'test.slda.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis does not have a module that allows topic models estimated using Tomotopy to be used directly for plotting the graphs. It however allows plotting after the following parameters are computed for each of the topic models:-\n",
    "\n",
    "1. phi\n",
    "\n",
    "    a. probabilities of each word(W) for a given topic(K) under consideration\n",
    "    \n",
    "    b. is a K x W vector\n",
    "    \n",
    "    \n",
    "2. theta\n",
    "\n",
    "    a. probability mass function over \"K\" topics for all the documents in the corpus (D)\n",
    "    \n",
    "    b. is a D x K matrix\n",
    "    \n",
    "    \n",
    "3. n(d)\n",
    "\n",
    "    a. number of tokens for each document\n",
    "\n",
    "\n",
    "4. vocab\n",
    "\n",
    "    a. vector of terms in the vocabulary\n",
    "    \n",
    "    b. presented in the same order as in \"phi\"\n",
    "    \n",
    "    \n",
    "5. M(w)\n",
    "\n",
    "    a. frequency of term \"w\" across the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the value of \"Phi\" for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi(model):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of phi for visualising the results of topic model\n",
    "        probabilities of each word for a given topic\n",
    "    input:\n",
    "        the topic model\n",
    "    output:\n",
    "        K x W vector\n",
    "        K = number of topics\n",
    "        W = number of words\n",
    "    \"\"\"\n",
    "    mat_phi1 = []\n",
    "    for i in range(model.k):\n",
    "        #print(model.get_topic_words(i,model.num_vocabs))\n",
    "        mat_phi1.append(model.get_topic_words(i,model.num_vocabs))\n",
    "    \n",
    "    list_words = []\n",
    "    for text in mat_phi1[0]:\n",
    "            list_words.append(text[0])\n",
    "    \n",
    "    #print(list_words)\n",
    "    list_words.sort()\n",
    "    #print(list_words)\n",
    "    \n",
    "    mat_phi2 = [[i * j for j in range(model.num_vocabs)] for i in range(model.k+1)]\n",
    "    for i in range(model.num_vocabs):\n",
    "        mat_phi2[0][i] = list_words[i]\n",
    "\n",
    "    \n",
    "    j1 = []\n",
    "    k1 = []\n",
    "    m = 0\n",
    "    while m < model.k:\n",
    "        j1.append(m)\n",
    "        m += 1\n",
    "        \n",
    "    n = 1\n",
    "    while n <= model.k:\n",
    "        k1.append(n)\n",
    "        n += 1\n",
    "        \n",
    "    for j, k in zip(j1, k1):\n",
    "        for index, word in enumerate(mat_phi2[0]):\n",
    "            #print(word)\n",
    "            for item in mat_phi1[j]:\n",
    "                #print(item)\n",
    "                if word == item[0]:\n",
    "                    mat_phi2[k][index] = item[1]\n",
    "    \n",
    "    if os.path.isfile(directo + '\\\\topic_word_prob_lda.csv'):\n",
    "        with open(directo + '\\\\topic_word_prob_slda.csv', 'w') as f:\n",
    "            for item in mat_phi2:\n",
    "                for items in item:\n",
    "                    f.writelines(\"%s, \" % items)\n",
    "                f.writelines(\"\\n\")\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(directo + '\\\\topic_word_prob_lda.csv', 'w') as f:\n",
    "            for item in mat_phi2:\n",
    "                for items in item:\n",
    "                    f.writelines(\"%s, \" % items)\n",
    "                f.writelines(\"\\n\")\n",
    "            f.close()\n",
    "        \n",
    "    return mat_phi2[0], mat_phi2[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the value of \"Theta\" for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_lda(model, data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of theta for visualising the results of topic model\n",
    "        probabilities mass function over \"K\" topics for all documents (D) in the corpus\n",
    "    input:\n",
    "        the topic model\n",
    "        dataset\n",
    "    output:\n",
    "        D x K vector\n",
    "        D = number of documents\n",
    "        K = number of topics\n",
    "    \"\"\"\n",
    "    mat_theta = []\n",
    "    for n, line in enumerate(data):\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        theta_val = model.infer(doc=model.make_doc(docu),\n",
    "                                     iter=100,\n",
    "                                     workers=0,\n",
    "                                     together=False)\n",
    "        mat_theta.append(theta_val[0])\n",
    "    \n",
    "    with open(directo + '\\\\topic_probabilities_lda.csv', 'w') as f:\n",
    "        for item in mat_theta:\n",
    "            for items in item:\n",
    "                f.writelines(\"%s, \" %items)\n",
    "            f.writelines(\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return mat_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For sLDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_slda(model, data, dep_var):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of theta for visualising the results of topic model\n",
    "        probabilities mass function over \"K\" topics for all documents (D) in the corpus\n",
    "    input:\n",
    "        the topic model\n",
    "        dataset\n",
    "        dependent variable\n",
    "    output:\n",
    "        D x K vector\n",
    "        D = number of documents\n",
    "        K = number of topics\n",
    "    \"\"\"\n",
    "    mat_theta = []\n",
    "    for line, dep in zip(data, dep_var):\n",
    "        pred_1 = []\n",
    "        pred_1.append(dep)\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        theta_val = model.infer(doc=model.make_doc(words=docu, y=pred_1),\n",
    "                                     iter=100,\n",
    "                                     workers=0,\n",
    "                                     together=False)\n",
    "        mat_theta.append(theta_val[0])\n",
    "        \n",
    "    with open(directo + '\\\\topic_probabilities_slda.csv', 'w') as f:\n",
    "        for item in mat_theta:\n",
    "            for items in item:\n",
    "                f.writelines(\"%s, \" %items)\n",
    "            f.writelines(\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return mat_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tokens per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_token(data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes number of tokens per document for the entire corpus\n",
    "    input:\n",
    "        dataset\n",
    "    output:\n",
    "        N x 1 vector\n",
    "        N = number of tokens in the document\n",
    "    \"\"\"\n",
    "    numb_tok = []\n",
    "    for text in data:\n",
    "        numb_tok.append(len(text))\n",
    "    return numb_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(vocabs, data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the frequency of words in the entire corpus\n",
    "    input:\n",
    "        list of words\n",
    "        dataset\n",
    "    output:\n",
    "        N x 1 vector\n",
    "        N = frequency of words in the document\n",
    "    \"\"\"\n",
    "    fre_words = []\n",
    "    for words in vocabs:\n",
    "        words_freq = 0\n",
    "        for line in data:\n",
    "            for ind_words in line:\n",
    "                if words == ind_words:\n",
    "                    words_freq += 1\n",
    "        fre_words.append(words_freq)\n",
    "    return fre_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Parameters for Visualising LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LDA model\n",
    "lda_model = tp.LDAModel.load('test.lda.bin')\n",
    "#lda_model.get_topic_word_dist(2)\n",
    "lvocab, lphi_val = compute_phi(lda_model)\n",
    "ltheta_val = compute_theta_lda(lda_model, data_lemmatized)\n",
    "lnum_token = num_token(data_lemmatized)\n",
    "lfreq_terms = freq_words(lvocab, data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting in pyLDAvis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the Results\n",
    "pyLDAvis.enable_notebook()\n",
    "data_lda = {'topic_term_dists': lphi_val,\n",
    "            'doc_topic_dists' : ltheta_val,\n",
    "            'doc_lengths'     : lnum_token,\n",
    "            'vocab'           : lvocab,\n",
    "            'term_frequency'  : lfreq_terms}\n",
    "print('Topic-Term shape: %s' % str(np.array(data_lda['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(data_lda['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_lda = pyLDAvis.prepare(**data_lda)\n",
    "pyLDAvis.display(vis_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of Supervised LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Parameters for Visualising Supervised LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sLDA model\n",
    "slda_model = tp.SLDAModel.load('test.slda.bin')\n",
    "#slda_model.get_topic_word_dist(2)\n",
    "svocab, sphi_val = compute_phi(slda_model)\n",
    "stheta_val = compute_theta_slda(slda_model, data_lemmatized, resp)\n",
    "snum_token = num_token(data_lemmatized)\n",
    "sfreq_terms = freq_words(svocab, data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting in pyLDAvis (sLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Results\n",
    "pyLDAvis.enable_notebook()\n",
    "data_slda = {'topic_term_dists': sphi_val,\n",
    "             'doc_topic_dists' : stheta_val,\n",
    "             'doc_lengths'     : snum_token,\n",
    "             'vocab'           : svocab,\n",
    "             'term_frequency'  : sfreq_terms}\n",
    "print('Topic-Term shape: %s' % str(np.array(data_slda['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(data_slda['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_slda = pyLDAvis.prepare(**data_slda)\n",
    "pyLDAvis.display(vis_slda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Scores for use in Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the code, values are computed for each document in the corpus. The values are computed based on the words used in each of the documents in the corpus. Scores will be computed for each topic. This will be based on the probability values in each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(list_dataset, list_word_prob):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function will take the cleaned dataset and list of word probabilities per topic and compute the scores\n",
    "    input:\n",
    "        cleaned dataset as a list\n",
    "        word probabilities as a dataframe\n",
    "    output:\n",
    "        scores for each document in the corpus\n",
    "    \"\"\"\n",
    "    n = len(list_dataset)\n",
    "    prob_list = [[0 for i in range(4)] for i in range(n)]\n",
    "    for index, document in enumerate(list_dataset):\n",
    "        # remember to change the number of variables based on the number of topics\n",
    "        probab_1 = 0\n",
    "        probab_2 = 0\n",
    "        probab_3 = 0\n",
    "        for word in document:\n",
    "            for index1, row in list_word_prob.iterrows():\n",
    "                item  = row['Word']\n",
    "                prob1 = row['Prob_1']\n",
    "                prob2 = row['Prob_2']\n",
    "                prob3 = row['Prob_3']\n",
    "                if word == item:\n",
    "                    probab_1 += prob1\n",
    "                    probab_2 += prob2\n",
    "                    probab_3 += prob3\n",
    "            \n",
    "            prob_list[index][0] = probab_1\n",
    "            prob_list[index][1] = probab_2\n",
    "            prob_list[index][2] = probab_3\n",
    "            prob_list[index][3] = probab_1 + probab_2 + probab_3\n",
    "    \n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Scores for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dist = pd.read_csv(directo + \"\\\\topic_word_prob_lda.csv\", header=None)\n",
    "lda_distT = lda_dist.T\n",
    "lda_distT.columns = ['Word', 'Prob_1', 'Prob_2', 'Prob_3']\n",
    "lda_distT['Word']   = lda_distT['Word'].str.strip()\n",
    "lda_distT['Prob_1'] = pd.to_numeric(lda_distT.Prob_1, errors='coerce')\n",
    "lda_distT['Prob_2'] = pd.to_numeric(lda_distT.Prob_2, errors='coerce')\n",
    "lda_distT['Prob_3'] = pd.to_numeric(lda_distT.Prob_3, errors='coerce')\n",
    "probab_lda = compute_scores(data_lemmatized, lda_distT)\n",
    "df['probab_lda'] = probab_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Scores for sLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slda_dist = pd.read_csv(directo + \"\\\\topic_word_prob_slda.csv\", header=None)\n",
    "slda_distT = slda_dist.T\n",
    "slda_distT.columns = ['Word', 'Prob_1', 'Prob_2', 'Prob_3']\n",
    "slda_distT['Word']   = slda_distT['Word'].str.strip()\n",
    "slda_distT['Prob_1'] = pd.to_numeric(slda_distT.Prob_1, errors='coerce')\n",
    "slda_distT['Prob_2'] = pd.to_numeric(slda_distT.Prob_2, errors='coerce')\n",
    "slda_distT['Prob_3'] = pd.to_numeric(slda_distT.Prob_3, errors='coerce')\n",
    "probab_slda = compute_scores(data_lemmatized, slda_distT)\n",
    "df['probab_slda'] = probab_slda\n",
    "\n",
    "df.to_csv(directo + \"\\\\Open_Ended_Q1_Scores_Calibration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
