{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is written to extract topics from text data. The code supports performing the following analysis:-\n",
    "1. Latent Dirichlet Allocation (LDA)\n",
    "2. Supervised Latent Dirichlet Allocation (sLDA)\n",
    "\n",
    "Latent Dirichlet allocation can be performed using Gensim or Tomotopy. Supervised LDA can be performed using Tomotopy. The dependent variable can be linear or binary in nature.\n",
    "\n",
    "In addition to this, the code also allows users to evaluate models using measures such as Coherence and Perplexity. Various visualisations can also be used to evaluate the results from the topic models. These include:-\n",
    "1. pyLDAvis to understand topics and the inter-topic distance\n",
    "2. Word clouds for topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ************************** Importing Packages ************************ ###\n",
    "from __future__ import division\n",
    "import re                     # regular expressions\n",
    "import numpy as np            # scientific computing\n",
    "import pandas as pd           # datastructures and computing\n",
    "import pprint as pprint       # better printing\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Plotting tools\n",
    "#import graphlab as gl\n",
    "import pyLDAvis               # interactive topic model visualisation\n",
    "#import pyLDAvis.graphlab\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline          # to ensure that the matplotlib plots are printed in the Jupyter notebooks\n",
    "\n",
    "# Libraries for Topic Models\n",
    "import sys\n",
    "import tomotopy as tp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the list of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['app', 'apps', 'also', 'android', 'atm', 'atms', 'call', 'calls', 'calling', 'browsing', 'browse', \n",
    "                   'contact','clock', 'communication', 'dk', 'edu', 'e-mails', 'email', 'emails', 'etc', 'etfc', \n",
    "                   'entertainment', 'fb', 'files', 'from', 'food', 'images', 'info', 'internet', 'jpg', 'online', \n",
    "                   'mail', 'make', 'much', 'mean', 'music', 'messaging', 'messenger', 'mobilepay', 'nd', 'networks', \n",
    "                   'news', 'parents', 'friends', 'pdf', 'player', 'photos', 'photographs', 'photography', 'receiving', \n",
    "                   're', 'related', 'reviews', 'rooms', 'social', 'subject', 'sms', 'storage', 'skype', 'twitter', \n",
    "                   'text', 'texting', 'use', 'using', 'www', 'wifi'])\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset preparation is quite important here. If the model used is simple LDA, only the text data is mandatory.\n",
    "\n",
    "However, for the supervised LDA, it requires the response variable (dependent variable) along with the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google Maps,Redbus,IRCTC,Make My Trip,Goibibo'\n",
      " 'Booking tickets online, Tracking where am I (if i am traveling in a place which not so familiar) in google maps, check the time to reach my destination'\n",
      " 'Google maps, bus timing, train booking' 'Maps, ticket booking'\n",
      " 'Locating destination  Taxi booking' 'Maps,hotel booking'\n",
      " 'Book travel (Ola, Uber, Train, Flights and Bus)  Book food while on travel  Book hotels'\n",
      " 'shortest route to destination and time taken  booking of travel tickets'\n",
      " 'knowing the status of train    '\n",
      " 'Navigation, finding eateries, hotels,   Banking while travels  News  Entertainment  '\n",
      " '1 . GPS  2 . Online reservations ' 'Booking cabs'\n",
      " 'Find routes. Book cab. Find train times'\n",
      " 'GPS  GOOGLE MAP & NAVIGATOR   TO FIND TOURIST SPOTS'\n",
      " 'For navigation and photography ' 'Maps, Booking tickets'\n",
      " 'Reviews about the place,route map.' 'Navigation, bookings'\n",
      " 'Taxi booking, train timings, airline check in '\n",
      " 'Ticket Booking   Online check in'\n",
      " 'For checking schedules, availability of tickets.  For communication '\n",
      " '1. Navigation   2. Public transport apps, skyscanner' 'Book room,ticket'\n",
      " 'Booking air tickets and car rentals'\n",
      " 'Google Maps for Navigation    To browse nearby restaurants'\n",
      " 'To book flights and hotel. To make the travel itinerary'\n",
      " '1. Navigation (Google Maps)  2. Booking' 'Taxi, Map'\n",
      " '1. Reservations   2. Maps' 'Cab booking, ticket booking '\n",
      " 'To know the area using google maps, to book tickets and find deals on bookings, to book hotel rooms'\n",
      " 'Book train,  bus tickets  Online taxi'\n",
      " 'Maps, Flight-train-taxi reservation, Places to visit'\n",
      " 'Online booking,  checking status '\n",
      " 'Navigate to exact location  Getting reviews on various places'\n",
      " 'Ticket Bookings, Can Bookings'\n",
      " 'Travel for booking cabs in ola, uber etc  Also booking for trains ticket fast with some of apps , train running status from Indian rails info , IRCTC android app'\n",
      " '1. Google map for finding directions  2. Booking tickets and hotels'\n",
      " 'Booking tickets  Navigation  Hotel availability  Atm location  Train tracking  Online food order  Leisure during journey'\n",
      " 'Reservation and google map' 'Google mapping and uber cabs.'\n",
      " 'To know the distance and shortest route.  To know the most economic mode of travel  To keep the route intact while on drive'\n",
      " '1) To find location and route.  2) To book travel tickets.  3) To find top sights and schedule travel.'\n",
      " 'Google maps, hotel booking apps, travel apps with ratings, '\n",
      " 'Google Map to navigate my location' 'Google Maps, online transactions'\n",
      " 'Finding routes and booking tickets' 'Booking tickets & accomadation  '\n",
      " '1Google map  2 find required services'\n",
      " 'Booking tickets.  To check live running status.'\n",
      " 'personal vehicle-location, map  public transport - ticket booking, payment, running status  '\n",
      " '1. Map services, and locating restaurants and other landmark.  2. As a wallet, booking uber, booking hotel rooms etc  3. Communication '\n",
      " 'Rout map and booking hotels.'\n",
      " 'Sometimes to see where I am right now, and sometimes for directions. I use Google maps for both purposes. On a related note, I like talking to my parents or friends while traveling, so I call or text them frequently. '\n",
      " 'Maps, cab booking' 'Maps,GPS.online reservation ,'\n",
      " 'gps- location positioning  google maps'\n",
      " 'Navigation and Route search  Ticket Booking/ Cab booking'\n",
      " 'Online booking, maps '\n",
      " '1. Navigation.  2. Realtime traffic updates.  3. Shopping/ reservations.  4. Exploring options based on location.'\n",
      " 'Online taxi, railway bookings'\n",
      " 'Booking tickets for movie  Online shopping'\n",
      " '1. Search for public transport in apps  2. Booking tickets   '\n",
      " 'GPS Service, Online ticket reservation.'\n",
      " 'Navigation and finding hotels and food' 'No que required  Safety'\n",
      " 'To book cabs. To find route.'\n",
      " 'Reserve rail ticket    Look at train timings' 'Messages   Bookings'\n",
      " 'Buying ticket online' 'Route maps, bookings and reservations'\n",
      " '1. Easiness and convenience to take the ride for a travel.  2. To track the route we travel.'\n",
      " 'For directions  For knowing about the places about to visit  How to reach the place    '\n",
      " 'Booking taxi  Booking bus tickets  Finding route'\n",
      " '1. Booking the ticket.  2. Track the vehicle.  3. See the location  4. To check ETA'\n",
      " 'booking,maps' 'GPS,internet' 'Calling, Browsing'\n",
      " 'Booking flights   Online check in'\n",
      " 'Online booking of tickets  Transport service timings  '\n",
      " 'Flight fares  Vacation options'\n",
      " 'Checking the best rates and booking tickets and hotel bookings'\n",
      " 'Only e bookings, mobile apps for flight charges'\n",
      " 'Ticket booking and tracking' 'Maps - Navigation, Nearest places'\n",
      " 'Call, browse ' 'Finding atm restaurant petrol pump  Google map'\n",
      " 'GPS, online booking'\n",
      " 'Navigating, Checking traffic status before starting travel'\n",
      " 'Navigation, whether updates , ticket reservations'\n",
      " 'Root map via app,capturing images' 'Maps, Alerts, Tickets'\n",
      " '1. Book train tickets.  2  Plan journey using maps.'\n",
      " 'Location access  To collect details of nearest heritage'\n",
      " 'Checking running status of train.  Checklist of items.  Online reservation'\n",
      " '1.BOOKING TICKETS THROUGH APPS AND SIMULTANEOUSLY EXPECTING OFFERS FOR TICKET BOOKING USING DIFFERENT APPS   2.GOOGLE MAP APP'\n",
      " 'Navigation  Route calculation  Music playback through Infotainment connection with smartphone'\n",
      " 'Using maps to find directions  Listening to music'\n",
      " 'Use navigation apps such as Google map  For ticket related use.. '\n",
      " 'Google map  Train information' '1. Maps  2. to find tourist places'\n",
      " 'Route  Update traffic  Look for train current location'\n",
      " 'To use Google maps and search about the places'\n",
      " 'To find route. To know my location. '\n",
      " 'To book online cabs, tickets etc  Use Google maps for going unknown places'\n",
      " 'Google map'\n",
      " 'Navigation via google map  Favourite places  Location searching  Earth features  Engineering constructions  Traffic related helps  Photography and videography  Communication'\n",
      " 'Booking my tickets and also uses the google map  '\n",
      " 'For booking tickets  Using google map'\n",
      " '1.Google map  2.Knowing Weather of place'\n",
      " 'Google map to locate place  Use smartphone to find restaurants ATMS etc  '\n",
      " '1.finding the way to reach destinations  2.to find lodges and restraunts nearby'\n",
      " 'Entertainment during travel  Contact with others'\n",
      " 'Ticket bookings  Online taxi' 'Maps' '1) Navigation   2)Booking hotels '\n",
      " 'Route guidance  Locating vantage points such fuel refilling station, bus stops etc.'\n",
      " '1. I book online taxi very often  2. I book online travel tickets. In fact I never book tickets offline'\n",
      " 'Payment of fares, booking tickets'\n",
      " 'For booking tickets, to spot location,to check seat availability, to compare travel fare etc'\n",
      " 'Navigation   Booking tickets' 'Finding easy way and traffic alert s'\n",
      " 'Hotel reservation  Directions'\n",
      " 'Booking tickets, navigation, photography'\n",
      " 'For location and onlinepayment'\n",
      " 'Location Searching  Use of prepaid taxi  Shortest Routes  Traffic Status'\n",
      " 'Finding hotels and flights  Searching for major attractions  Finding routes in Google Maps'\n",
      " 'Booking ticket and hotels, Searching rourtes (Maping)'\n",
      " 'Ticket bookings, maps, online booking of hotels '\n",
      " 'Checking availability of transport mode  Route guidance'\n",
      " 'Maps and online booking ' '1 Google Map  2 online taxi '\n",
      " 'For exact Location determination  Distance calculation' 'Maps,book cab'\n",
      " 'Finding places, restaurants ' 'Planning, travelling itself'\n",
      " 'Book flights, book hotel rooms & navigation'\n",
      " 'Navigation and ticket booking'\n",
      " 'For my convienence and for user friendly '\n",
      " \"Real-time traffic information to plan travel better and make decisions on the go  Find what's near me while traveling to easily find what I need on the go  If you are traveling by air/rail/bus/or any public transportation means, to know the schedule, delays and also use a digital copy of your travel ticket  \"\n",
      " '1. GPS location search  2. Cab booking' 'Map the route  Booking tickets'\n",
      " '1. Booking flight tickets  2. Booking taxi  3. Use maps for navigation'\n",
      " 'Gps, online ticket booking' '1) Navigation  2) Booking cabs  ']\n"
     ]
    }
   ],
   "source": [
    "### ************************** Importing Datasets ************************ ###\n",
    "directo = \"C:\\\\Users\\\\vibabu\\\\Dropbox\\\\Doctoral_Research\\\\STSM\\\\Analysis\\\\Combined_Dataset\\\\SEM_October_19\\\\India_OE\\\\Results_Feb_10\\\\Open_Ended_Q1\"\n",
    "\n",
    "df = pd.read_csv(directo + \"\\\\Open_Ended_Q1_90_pct.csv\", encoding='latin-1')\n",
    "print(df.Response.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6664092292</td>\n",
       "      <td>1</td>\n",
       "      <td>Google Maps,Redbus,IRCTC,Make My Trip,Goibibo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6624275913</td>\n",
       "      <td>4</td>\n",
       "      <td>Booking tickets online, Tracking where am I (i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6623765999</td>\n",
       "      <td>3</td>\n",
       "      <td>Google maps, bus timing, train booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6623636399</td>\n",
       "      <td>2</td>\n",
       "      <td>Maps, ticket booking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6613595569</td>\n",
       "      <td>2</td>\n",
       "      <td>Locating destination  Taxi booking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Rating                                           Response\n",
       "0  6664092292       1      Google Maps,Redbus,IRCTC,Make My Trip,Goibibo\n",
       "1  6624275913       4  Booking tickets online, Tracking where am I (i...\n",
       "2  6623765999       3             Google maps, bus timing, train booking\n",
       "3  6623636399       2                               Maps, ticket booking\n",
       "4  6613595569       2                 Locating destination  Taxi booking"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google Maps,Redbus,IRCTC,Make My Trip,Goibibo',\n",
       " 'Booking tickets online, Tracking where am I (if i am traveling in a place which not so familiar) in google maps, check the time to reach my destination',\n",
       " 'Google maps, bus timing, train booking',\n",
       " 'Maps, ticket booking',\n",
       " 'Locating destination  Taxi booking']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the content field in dataset into a list\n",
    "data = df.Response.values.tolist()\n",
    "resp = df.Rating.values.tolist()\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3, 2, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code, the following data cleaning techniques are used:-\n",
    "\n",
    "1. E-mail id  and New line characters\n",
    "2. Remove \"StopWords\" from the dataset\n",
    "3. Forming bigrams and trigrams\n",
    "4. Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing emails and new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google Maps,Redbus,IRCTC,Make My Trip,Goibibo',\n",
      " 'Booking tickets online, Tracking where am I (if i am traveling in a place '\n",
      " 'which not so familiar) in google maps, check the time to reach my '\n",
      " 'destination',\n",
      " 'Google maps, bus timing, train booking',\n",
      " 'Maps, ticket booking',\n",
      " 'Locating destination  Taxi booking']\n"
     ]
    }
   ],
   "source": [
    "### ************************** Datasets Cleaning ************************* ###\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\n', ' ', sent) for sent in data]\n",
    "\n",
    "pprint.pprint(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove StopWords\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "        \"\"\"\n",
    "        objective:\n",
    "            function to remove stopwords from the paragraph/sentence\n",
    "            uses the preprocess\n",
    "        input:\n",
    "            paragraph/sentences\n",
    "        output:\n",
    "            wordlist after the stopwords are removed\n",
    "        \"\"\"\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "             for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['google', 'maps', 'redbus', 'irctc', 'trip', 'goibibo'],\n",
       " ['booking',\n",
       "  'tickets',\n",
       "  'tracking',\n",
       "  'traveling',\n",
       "  'place',\n",
       "  'familiar',\n",
       "  'google',\n",
       "  'maps',\n",
       "  'check',\n",
       "  'time',\n",
       "  'reach',\n",
       "  'destination'],\n",
       " ['google', 'maps', 'bus', 'timing', 'train', 'booking'],\n",
       " ['maps', 'ticket', 'booking'],\n",
       " ['locating', 'destination', 'taxi', 'booking']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data)\n",
    "data_words_nostops[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Forming Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    \"\"\"\n",
    "    objective:\n",
    "        takes the processed text- after preprocessing and stop word removal\n",
    "    input:\n",
    "        preprocessed text\n",
    "    output:\n",
    "        text with bigrams\n",
    "    \"\"\"\n",
    "    return [bigram_mod[text] for text in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    \"\"\"\n",
    "    objective:\n",
    "        generate trigrams for the text\n",
    "    input:\n",
    "        text with bigrams\n",
    "    output:\n",
    "        text with trigrams    \n",
    "    \"\"\"\n",
    "    return [trigram_mod[bigram_mod[text]] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['google', 'maps', 'redbus', 'irctc', 'trip', 'goibibo'],\n",
       " ['booking',\n",
       "  'tickets',\n",
       "  'tracking',\n",
       "  'traveling',\n",
       "  'place',\n",
       "  'familiar',\n",
       "  'google',\n",
       "  'maps',\n",
       "  'check',\n",
       "  'time',\n",
       "  'reach',\n",
       "  'destination'],\n",
       " ['google', 'maps', 'bus', 'timing', 'train', 'booking'],\n",
       " ['maps', 'ticket', 'booking'],\n",
       " ['locating', 'destination', 'taxi', 'booking']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calibration Dataset\n",
    "\n",
    "# Build functions to remove stopwords, bigram and trigram models- calibration dataset\n",
    "bigram = gensim.models.phrases.Phrases(data, min_count=5, threshold=100)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[data], threshold=100)\n",
    "\n",
    "# Passing the parameters to the bigram/trigram- calibration dataset\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_bigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['googl', 'map', 'redbu', 'irctc', 'trip', 'goibibo'],\n",
       " ['book',\n",
       "  'ticket',\n",
       "  'track',\n",
       "  'travel',\n",
       "  'place',\n",
       "  'familiar',\n",
       "  'googl',\n",
       "  'map',\n",
       "  'check',\n",
       "  'time',\n",
       "  'reach',\n",
       "  'destin'],\n",
       " ['googl', 'map', 'bu', 'time', 'train', 'book'],\n",
       " ['map', 'ticket', 'book'],\n",
       " ['locat', 'destin', 'taxi', 'book']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "data_lemmatized = []\n",
    "for texts in data_words_bigrams:\n",
    "    data_lemmatized.append([ps.stem(doc) for doc in texts])\n",
    "    \n",
    "data_lemmatized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Files to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_Data'] = data_lemmatized\n",
    "df.head()\n",
    "df.to_csv(directo + \"\\\\Output_Q1_Words_Calibration.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LDA Function\n",
    "    \n",
    "def lda_model(input_list, save_path):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        the function estimates the LDA model and outputs the estimated topics\n",
    "    input:\n",
    "        list with documents as responses\n",
    "    output:\n",
    "        prints the topics\n",
    "        words and their corresponding proportions\n",
    "    \"\"\"\n",
    "    mdl = tp.LDAModel(tw=tp.TermWeight.ONE,             # Term weighting\n",
    "                      min_cf=3,                         # Minimum frequency of words\n",
    "                      rm_top=0,                         # Number of top frequency words to be removed\n",
    "                      k=3)                              # Number of topics\n",
    "    for n, line in enumerate(input_list):\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        mdl.add_doc(docu)\n",
    "    mdl.burn_in = 100\n",
    "    mdl.train(0)\n",
    "    print('Num docs: ', len(mdl.docs), 'Vocab size: ', mdl.num_vocabs, 'Num words: ', mdl.num_words)\n",
    "    print('Removed words: ', mdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        mdl.train(1000)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "        \n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    mdl.save(save_path, True)\n",
    "    \n",
    "    for k in range(mdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in mdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LDA\n",
      "Num docs:  143 Vocab size:  42 Num words:  577\n",
      "Removed words:  []\n",
      "Iteration: 0\tLog-likelihood: -3.6185059650208755\n",
      "Iteration: 10\tLog-likelihood: -3.6328995266293322\n",
      "Iteration: 20\tLog-likelihood: -3.6750923707383962\n",
      "Iteration: 30\tLog-likelihood: -3.643572142854514\n",
      "Iteration: 40\tLog-likelihood: -3.6101904176240156\n",
      "Iteration: 50\tLog-likelihood: -3.549582522084849\n",
      "Iteration: 60\tLog-likelihood: -3.5713083892027675\n",
      "Iteration: 70\tLog-likelihood: -3.661895139972507\n",
      "Iteration: 80\tLog-likelihood: -3.5634897314722243\n",
      "Iteration: 90\tLog-likelihood: -3.590429211813356\n",
      "Iteration: 100\tLog-likelihood: -3.684176232904661\n",
      "Iteration: 110\tLog-likelihood: -3.660799719832282\n",
      "Iteration: 120\tLog-likelihood: -3.6109065015249895\n",
      "Iteration: 130\tLog-likelihood: -3.7042781510672804\n",
      "Iteration: 140\tLog-likelihood: -3.654083575026828\n",
      "Iteration: 150\tLog-likelihood: -3.5632668006616406\n",
      "Iteration: 160\tLog-likelihood: -3.6729990164370014\n",
      "Iteration: 170\tLog-likelihood: -3.6446837721912244\n",
      "Iteration: 180\tLog-likelihood: -3.6573239467792744\n",
      "Iteration: 190\tLog-likelihood: -3.5797448868974655\n",
      "Iteration: 200\tLog-likelihood: -3.570544790715444\n",
      "Iteration: 210\tLog-likelihood: -3.652273561631166\n",
      "Iteration: 220\tLog-likelihood: -3.552833283897909\n",
      "Iteration: 230\tLog-likelihood: -3.670235598469524\n",
      "Iteration: 240\tLog-likelihood: -3.654827841026531\n",
      "Iteration: 250\tLog-likelihood: -3.6662465014415866\n",
      "Iteration: 260\tLog-likelihood: -3.6019679657998696\n",
      "Iteration: 270\tLog-likelihood: -3.627208692310588\n",
      "Iteration: 280\tLog-likelihood: -3.588577967479283\n",
      "Iteration: 290\tLog-likelihood: -3.655335791150763\n",
      "Iteration: 300\tLog-likelihood: -3.630779555238512\n",
      "Iteration: 310\tLog-likelihood: -3.679798053177251\n",
      "Iteration: 320\tLog-likelihood: -3.541898711508647\n",
      "Iteration: 330\tLog-likelihood: -3.655280734200928\n",
      "Iteration: 340\tLog-likelihood: -3.5895185470839306\n",
      "Iteration: 350\tLog-likelihood: -3.665060726593231\n",
      "Iteration: 360\tLog-likelihood: -3.6817359873569817\n",
      "Iteration: 370\tLog-likelihood: -3.6372532542388196\n",
      "Iteration: 380\tLog-likelihood: -3.5918297211363575\n",
      "Iteration: 390\tLog-likelihood: -3.646715427085473\n",
      "Iteration: 400\tLog-likelihood: -3.6977473378594663\n",
      "Iteration: 410\tLog-likelihood: -3.582273266111957\n",
      "Iteration: 420\tLog-likelihood: -3.609748829730772\n",
      "Iteration: 430\tLog-likelihood: -3.593309330981567\n",
      "Iteration: 440\tLog-likelihood: -3.5244071378245097\n",
      "Iteration: 450\tLog-likelihood: -3.6638064775906463\n",
      "Iteration: 460\tLog-likelihood: -3.5712965002196295\n",
      "Iteration: 470\tLog-likelihood: -3.6482477182687054\n",
      "Iteration: 480\tLog-likelihood: -3.5706760484847906\n",
      "Iteration: 490\tLog-likelihood: -3.572443560166107\n",
      "Iteration: 500\tLog-likelihood: -3.5938579759833305\n",
      "Iteration: 510\tLog-likelihood: -3.646253815298477\n",
      "Iteration: 520\tLog-likelihood: -3.6665382286601504\n",
      "Iteration: 530\tLog-likelihood: -3.6520544199121265\n",
      "Iteration: 540\tLog-likelihood: -3.597142761418244\n",
      "Iteration: 550\tLog-likelihood: -3.643589166475644\n",
      "Iteration: 560\tLog-likelihood: -3.568590952264245\n",
      "Iteration: 570\tLog-likelihood: -3.643816872475672\n",
      "Iteration: 580\tLog-likelihood: -3.6787448550221\n",
      "Iteration: 590\tLog-likelihood: -3.6643953888678675\n",
      "Iteration: 600\tLog-likelihood: -3.6496923258544256\n",
      "Iteration: 610\tLog-likelihood: -3.6194610165454693\n",
      "Iteration: 620\tLog-likelihood: -3.6663879421079595\n",
      "Iteration: 630\tLog-likelihood: -3.6290256266403085\n",
      "Iteration: 640\tLog-likelihood: -3.6692417128143937\n",
      "Iteration: 650\tLog-likelihood: -3.6085556111003125\n",
      "Iteration: 660\tLog-likelihood: -3.6032873543440984\n",
      "Iteration: 670\tLog-likelihood: -3.6557363081897774\n",
      "Iteration: 680\tLog-likelihood: -3.6468345292002944\n",
      "Iteration: 690\tLog-likelihood: -3.6642426720834895\n",
      "Iteration: 700\tLog-likelihood: -3.6007906344115566\n",
      "Iteration: 710\tLog-likelihood: -3.609303073050451\n",
      "Iteration: 720\tLog-likelihood: -3.5662969585824467\n",
      "Iteration: 730\tLog-likelihood: -3.6005400440024546\n",
      "Iteration: 740\tLog-likelihood: -3.5875364811348516\n",
      "Iteration: 750\tLog-likelihood: -3.670125312212228\n",
      "Iteration: 760\tLog-likelihood: -3.5933016301286074\n",
      "Iteration: 770\tLog-likelihood: -3.6071191233432685\n",
      "Iteration: 780\tLog-likelihood: -3.723871165173099\n",
      "Iteration: 790\tLog-likelihood: -3.6280293385635423\n",
      "Iteration: 800\tLog-likelihood: -3.719792515917502\n",
      "Iteration: 810\tLog-likelihood: -3.6430941187942874\n",
      "Iteration: 820\tLog-likelihood: -3.644558012485504\n",
      "Iteration: 830\tLog-likelihood: -3.738943676653107\n",
      "Iteration: 840\tLog-likelihood: -3.6564439954098953\n",
      "Iteration: 850\tLog-likelihood: -3.6020643603657105\n",
      "Iteration: 860\tLog-likelihood: -3.603982116516996\n",
      "Iteration: 870\tLog-likelihood: -3.6216828203283935\n",
      "Iteration: 880\tLog-likelihood: -3.6492748752727047\n",
      "Iteration: 890\tLog-likelihood: -3.629973394786568\n",
      "Iteration: 900\tLog-likelihood: -3.605274222752457\n",
      "Iteration: 910\tLog-likelihood: -3.7695966019477547\n",
      "Iteration: 920\tLog-likelihood: -3.6546683591472524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 930\tLog-likelihood: -3.659135707174678\n",
      "Iteration: 940\tLog-likelihood: -3.7194633921882723\n",
      "Iteration: 950\tLog-likelihood: -3.675747972737152\n",
      "Iteration: 960\tLog-likelihood: -3.6495084011317127\n",
      "Iteration: 970\tLog-likelihood: -3.6223882290064977\n",
      "Iteration: 980\tLog-likelihood: -3.6350645303364644\n",
      "Iteration: 990\tLog-likelihood: -3.640377993961761\n",
      "Topic #0\n",
      "\t\tmap\t0.17195110023021698\n",
      "\t\tgoogl\t0.14330054819583893\n",
      "\t\tfind\t0.12420017272233963\n",
      "\t\trout\t0.10032470524311066\n",
      "\t\tlocat\t0.08122433722019196\n",
      "\t\tplace\t0.06689905375242233\n",
      "\t\treserv\t0.05734886974096298\n",
      "\t\tnavig\t0.05257377773523331\n",
      "\t\tgp\t0.04302358999848366\n",
      "\t\tsearch\t0.03824849799275398\n",
      "Topic #1\n",
      "\t\ttravel\t0.19430840015411377\n",
      "\t\ttrain\t0.12675224244594574\n",
      "\t\tcheck\t0.10986319929361343\n",
      "\t\ttime\t0.06764060258865356\n",
      "\t\tstatu\t0.06764060258865356\n",
      "\t\tknow\t0.05919608473777771\n",
      "\t\ttransport\t0.05075156316161156\n",
      "\t\ttrack\t0.0423070453107357\n",
      "\t\tlocat\t0.03386252745985985\n",
      "\t\tdestin\t0.03386252745985985\n",
      "Topic #2\n",
      "\t\tbook\t0.363429456949234\n",
      "\t\tticket\t0.2156776636838913\n",
      "\t\tmap\t0.08389905095100403\n",
      "\t\thotel\t0.07191918045282364\n",
      "\t\tnavig\t0.05993930250406265\n",
      "\t\tcab\t0.047959428280591965\n",
      "\t\ttaxi\t0.047959428280591965\n",
      "\t\tflight\t0.03597955405712128\n",
      "\t\tbu\t0.023999681696295738\n",
      "\t\tuber\t0.016013098880648613\n"
     ]
    }
   ],
   "source": [
    "print('Running LDA')\n",
    "lda_model = lda_model(data_lemmatized, 'test.lda.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slda_model(documents, dep_var, save_path):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        the function estimates the sLDA model and outputs the estimated topics\n",
    "    input:\n",
    "        list with documents as responses\n",
    "        dependent variable\n",
    "    output:\n",
    "        prints the topics\n",
    "        words and their corresponding proportions\n",
    "    \"\"\"\n",
    "    smdl = tp.SLDAModel(tw=tp.TermWeight.ONE,             # Term weighting\n",
    "                        min_cf=3,                         # Minimum frequency of words\n",
    "                        rm_top=0,                         # Number of top frequency words to be removed\n",
    "                        k=3,                              # Number of topics\n",
    "                        vars=['l'])                       # Number of dependent variables\n",
    "    for row, pred in zip(documents, dep_var):\n",
    "        pred_1 = []\n",
    "        pred_1.append(pred)\n",
    "        ch = \" \".join(row)\n",
    "        docu = ch.strip().split()\n",
    "        smdl.add_doc(words=docu, y=pred_1)\n",
    "        \n",
    "    smdl.burn_in = 100\n",
    "    smdl.train(0)\n",
    "    \n",
    "    # Printing the output statistics\n",
    "    print('Num docs: ', len(smdl.docs), 'Vocab size: ', smdl.num_vocabs, 'Num words: ', smdl.num_words)\n",
    "    print('Removed top words: ', smdl.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "    for i in range(0, 1000, 10):\n",
    "        smdl.train(1000)\n",
    "        print('Iteration: {}\\tLog-likelihood: {}'.format(i, smdl.ll_per_word))\n",
    "        \n",
    "    print('Saving...', file=sys.stderr, flush=True)\n",
    "    smdl.save(save_path, True)\n",
    "    \n",
    "    for k in range(smdl.k):\n",
    "        print('Topic #{}'.format(k))\n",
    "        for word, prob in smdl.get_topic_words(k):\n",
    "            print('\\t', word, prob, sep='\\t')\n",
    "    return smdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Supervised LDA\n",
      "Num docs:  143 Vocab size:  42 Num words:  577\n",
      "Removed top words:  []\n",
      "Iteration: 0\tLog-likelihood: -3.764507998975052\n",
      "Iteration: 10\tLog-likelihood: -3.7388537478386272\n",
      "Iteration: 20\tLog-likelihood: -3.718330618570296\n",
      "Iteration: 30\tLog-likelihood: -3.7325866818365494\n",
      "Iteration: 40\tLog-likelihood: -3.8479303536626737\n",
      "Iteration: 50\tLog-likelihood: -3.71068759553015\n",
      "Iteration: 60\tLog-likelihood: -3.7813160386335176\n",
      "Iteration: 70\tLog-likelihood: -3.7319791300691976\n",
      "Iteration: 80\tLog-likelihood: -3.7666003134382477\n",
      "Iteration: 90\tLog-likelihood: -3.7131842947121148\n",
      "Iteration: 100\tLog-likelihood: -3.7982367141078672\n",
      "Iteration: 110\tLog-likelihood: -3.8403937091043727\n",
      "Iteration: 120\tLog-likelihood: -3.753743842678335\n",
      "Iteration: 130\tLog-likelihood: -3.8167568571103714\n",
      "Iteration: 140\tLog-likelihood: -3.772952949969611\n",
      "Iteration: 150\tLog-likelihood: -3.7133875892479846\n",
      "Iteration: 160\tLog-likelihood: -3.798783546865658\n",
      "Iteration: 170\tLog-likelihood: -3.7401982605847914\n",
      "Iteration: 180\tLog-likelihood: -3.79867427887805\n",
      "Iteration: 190\tLog-likelihood: -3.785088009053706\n",
      "Iteration: 200\tLog-likelihood: -3.8106635214064424\n",
      "Iteration: 210\tLog-likelihood: -3.809141120328237\n",
      "Iteration: 220\tLog-likelihood: -3.776222090649643\n",
      "Iteration: 230\tLog-likelihood: -3.772264425198476\n",
      "Iteration: 240\tLog-likelihood: -3.7727675485962187\n",
      "Iteration: 250\tLog-likelihood: -3.77074745311313\n",
      "Iteration: 260\tLog-likelihood: -3.7767996964443546\n",
      "Iteration: 270\tLog-likelihood: -3.807311515330274\n",
      "Iteration: 280\tLog-likelihood: -3.76197927522649\n",
      "Iteration: 290\tLog-likelihood: -3.750218066908629\n",
      "Iteration: 300\tLog-likelihood: -3.7501915376601125\n",
      "Iteration: 310\tLog-likelihood: -3.8377992634869713\n",
      "Iteration: 320\tLog-likelihood: -3.8095473273361886\n",
      "Iteration: 330\tLog-likelihood: -3.794556346439484\n",
      "Iteration: 340\tLog-likelihood: -3.8226457688476456\n",
      "Iteration: 350\tLog-likelihood: -3.837418210072898\n",
      "Iteration: 360\tLog-likelihood: -3.83690204448107\n",
      "Iteration: 370\tLog-likelihood: -3.8594414284162264\n",
      "Iteration: 380\tLog-likelihood: -3.770517893807004\n",
      "Iteration: 390\tLog-likelihood: -3.8288180362535083\n",
      "Iteration: 400\tLog-likelihood: -3.8136613956189342\n",
      "Iteration: 410\tLog-likelihood: -3.80958792919929\n",
      "Iteration: 420\tLog-likelihood: -3.7959669496334665\n",
      "Iteration: 430\tLog-likelihood: -3.7701912062965732\n",
      "Iteration: 440\tLog-likelihood: -3.84207765088599\n",
      "Iteration: 450\tLog-likelihood: -3.8045309805391967\n",
      "Iteration: 460\tLog-likelihood: -3.7695166765997894\n",
      "Iteration: 470\tLog-likelihood: -3.7893217772744157\n",
      "Iteration: 480\tLog-likelihood: -3.840439462612998\n",
      "Iteration: 490\tLog-likelihood: -3.766328863525179\n",
      "Iteration: 500\tLog-likelihood: -3.80737193432943\n",
      "Iteration: 510\tLog-likelihood: -3.8282573339957064\n",
      "Iteration: 520\tLog-likelihood: -3.771836793547987\n",
      "Iteration: 530\tLog-likelihood: -3.83082713715674\n",
      "Iteration: 540\tLog-likelihood: -3.782334622667749\n",
      "Iteration: 550\tLog-likelihood: -3.816320436515166\n",
      "Iteration: 560\tLog-likelihood: -3.820247404122373\n",
      "Iteration: 570\tLog-likelihood: -3.8347092667897997\n",
      "Iteration: 580\tLog-likelihood: -3.8419097339350285\n",
      "Iteration: 590\tLog-likelihood: -3.775844193536601\n",
      "Iteration: 600\tLog-likelihood: -3.782086005621627\n",
      "Iteration: 610\tLog-likelihood: -3.8484375252779026\n",
      "Iteration: 620\tLog-likelihood: -3.7610827018450332\n",
      "Iteration: 630\tLog-likelihood: -3.7993894921578595\n",
      "Iteration: 640\tLog-likelihood: -3.8278071401338942\n",
      "Iteration: 650\tLog-likelihood: -3.777766173771636\n",
      "Iteration: 660\tLog-likelihood: -3.7672475795966713\n",
      "Iteration: 670\tLog-likelihood: -3.778664508676806\n",
      "Iteration: 680\tLog-likelihood: -3.833153249220955\n",
      "Iteration: 690\tLog-likelihood: -3.760041286419525\n",
      "Iteration: 700\tLog-likelihood: -3.6817255034603735\n",
      "Iteration: 710\tLog-likelihood: -3.8309588486840584\n",
      "Iteration: 720\tLog-likelihood: -3.8085947689518878\n",
      "Iteration: 730\tLog-likelihood: -3.8330808287317133\n",
      "Iteration: 740\tLog-likelihood: -3.8672208282825187\n",
      "Iteration: 750\tLog-likelihood: -3.8091262342292085\n",
      "Iteration: 760\tLog-likelihood: -3.832129016245007\n",
      "Iteration: 770\tLog-likelihood: -3.73331493140074\n",
      "Iteration: 780\tLog-likelihood: -3.738765937528384\n",
      "Iteration: 790\tLog-likelihood: -3.833178007168283\n",
      "Iteration: 800\tLog-likelihood: -3.7408985287807335\n",
      "Iteration: 810\tLog-likelihood: -3.7659237137874606\n",
      "Iteration: 820\tLog-likelihood: -3.776113469265014\n",
      "Iteration: 830\tLog-likelihood: -3.8011734303453393\n",
      "Iteration: 840\tLog-likelihood: -3.826326085859829\n",
      "Iteration: 850\tLog-likelihood: -3.8106653805906068\n",
      "Iteration: 860\tLog-likelihood: -3.8089405583650313\n",
      "Iteration: 870\tLog-likelihood: -3.808393163416013\n",
      "Iteration: 880\tLog-likelihood: -3.806085127783516\n",
      "Iteration: 890\tLog-likelihood: -3.833535492784978\n",
      "Iteration: 900\tLog-likelihood: -3.7572299639587055\n",
      "Iteration: 910\tLog-likelihood: -3.8191826350137954\n",
      "Iteration: 920\tLog-likelihood: -3.7860640584539818\n",
      "Iteration: 930\tLog-likelihood: -3.814340280218008\n",
      "Iteration: 940\tLog-likelihood: -3.8169980967632786\n",
      "Iteration: 950\tLog-likelihood: -3.7536962611820273\n",
      "Iteration: 960\tLog-likelihood: -3.757090035435298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 970\tLog-likelihood: -3.753612796607326\n",
      "Iteration: 980\tLog-likelihood: -3.8139419800313785\n",
      "Iteration: 990\tLog-likelihood: -3.7782158874850107\n",
      "Topic #0\n",
      "\t\tmap\t0.3136875033378601\n",
      "\t\tgoogl\t0.2767939567565918\n",
      "\t\tfind\t0.14766648411750793\n",
      "\t\tplace\t0.12921971082687378\n",
      "\t\trestaur\t0.0462091900408268\n",
      "\t\tdirect\t0.0462091900408268\n",
      "\t\treach\t0.027762405574321747\n",
      "\t\tdestin\t0.009315624833106995\n",
      "\t\ttravel\t9.223390225088224e-05\n",
      "\t\trout\t9.223390225088224e-05\n",
      "Topic #1\n",
      "\t\ttravel\t0.12969225645065308\n",
      "\t\tlocat\t0.1184195727109909\n",
      "\t\trout\t0.09587419778108597\n",
      "\t\ttrain\t0.0846015140414238\n",
      "\t\tcheck\t0.07332882285118103\n",
      "\t\tfind\t0.056419797241687775\n",
      "\t\ttime\t0.045147109776735306\n",
      "\t\tstatu\t0.045147109776735306\n",
      "\t\tknow\t0.045147109776735306\n",
      "\t\ttraffic\t0.03951076790690422\n",
      "Topic #2\n",
      "\t\tbook\t0.3112304210662842\n",
      "\t\tticket\t0.18470007181167603\n",
      "\t\tnavig\t0.08210792392492294\n",
      "\t\tmap\t0.07868818193674088\n",
      "\t\thotel\t0.06158949434757233\n",
      "\t\ttaxi\t0.04107106104493141\n",
      "\t\treserv\t0.04107106104493141\n",
      "\t\tcab\t0.04107106104493141\n",
      "\t\tflight\t0.030811846256256104\n",
      "\t\tgp\t0.030811846256256104\n"
     ]
    }
   ],
   "source": [
    "print('Running Supervised LDA')\n",
    "slda_model = slda_model(data_lemmatized, resp, 'test.slda.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis does not have a module that allows topic models estimated using Tomotopy to be used directly for plotting the graphs. It however allows plotting after the following parameters are computed for each of the topic models:-\n",
    "\n",
    "1. phi\n",
    "\n",
    "    a. probabilities of each word(W) for a given topic(K) under consideration\n",
    "    \n",
    "    b. is a K x W vector\n",
    "    \n",
    "    \n",
    "2. theta\n",
    "\n",
    "    a. probability mass function over \"K\" topics for all the documents in the corpus (D)\n",
    "    \n",
    "    b. is a D x K matrix\n",
    "    \n",
    "    \n",
    "3. n(d)\n",
    "\n",
    "    a. number of tokens for each document\n",
    "\n",
    "\n",
    "4. vocab\n",
    "\n",
    "    a. vector of terms in the vocabulary\n",
    "    \n",
    "    b. presented in the same order as in \"phi\"\n",
    "    \n",
    "    \n",
    "5. M(w)\n",
    "\n",
    "    a. frequency of term \"w\" across the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the value of \"Phi\" for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi(model):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of phi for visualising the results of topic model\n",
    "        probabilities of each word for a given topic\n",
    "    input:\n",
    "        the topic model\n",
    "    output:\n",
    "        K x W vector\n",
    "        K = number of topics\n",
    "        W = number of words\n",
    "    \"\"\"\n",
    "    mat_phi1 = []\n",
    "    for i in range(model.k):\n",
    "        #print(model.get_topic_words(i,model.num_vocabs))\n",
    "        mat_phi1.append(model.get_topic_words(i,model.num_vocabs))\n",
    "    \n",
    "    list_words = []\n",
    "    for text in mat_phi1[0]:\n",
    "            list_words.append(text[0])\n",
    "    \n",
    "    #print(list_words)\n",
    "    list_words.sort()\n",
    "    #print(list_words)\n",
    "    \n",
    "    mat_phi2 = [[i * j for j in range(model.num_vocabs)] for i in range(model.k+1)]\n",
    "    for i in range(model.num_vocabs):\n",
    "        mat_phi2[0][i] = list_words[i]\n",
    "\n",
    "    \n",
    "    j1 = []\n",
    "    k1 = []\n",
    "    m = 0\n",
    "    while m < model.k:\n",
    "        j1.append(m)\n",
    "        m += 1\n",
    "        \n",
    "    n = 1\n",
    "    while n <= model.k:\n",
    "        k1.append(n)\n",
    "        n += 1\n",
    "        \n",
    "    for j, k in zip(j1, k1):\n",
    "        for index, word in enumerate(mat_phi2[0]):\n",
    "            #print(word)\n",
    "            for item in mat_phi1[j]:\n",
    "                #print(item)\n",
    "                if word == item[0]:\n",
    "                    mat_phi2[k][index] = item[1]\n",
    "    \n",
    "    if os.path.isfile(directo + '\\\\topic_word_prob_lda.csv'):\n",
    "        with open(directo + '\\\\topic_word_prob_slda.csv', 'w') as f:\n",
    "            for item in mat_phi2:\n",
    "                for items in item:\n",
    "                    f.writelines(\"%s, \" % items)\n",
    "                f.writelines(\"\\n\")\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(directo + '\\\\topic_word_prob_lda.csv', 'w') as f:\n",
    "            for item in mat_phi2:\n",
    "                for items in item:\n",
    "                    f.writelines(\"%s, \" % items)\n",
    "                f.writelines(\"\\n\")\n",
    "            f.close()\n",
    "        \n",
    "    return mat_phi2[0], mat_phi2[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the value of \"Theta\" for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_lda(model, data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of theta for visualising the results of topic model\n",
    "        probabilities mass function over \"K\" topics for all documents (D) in the corpus\n",
    "    input:\n",
    "        the topic model\n",
    "        dataset\n",
    "    output:\n",
    "        D x K vector\n",
    "        D = number of documents\n",
    "        K = number of topics\n",
    "    \"\"\"\n",
    "    mat_theta = []\n",
    "    for n, line in enumerate(data):\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        theta_val = model.infer(doc=model.make_doc(docu),\n",
    "                                     iter=100,\n",
    "                                     workers=0,\n",
    "                                     together=False)\n",
    "        mat_theta.append(theta_val[0])\n",
    "    \n",
    "    with open(directo + '\\\\topic_probabilities_lda.csv', 'w') as f:\n",
    "        for item in mat_theta:\n",
    "            for items in item:\n",
    "                f.writelines(\"%s, \" %items)\n",
    "            f.writelines(\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return mat_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For sLDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta_slda(model, data, dep_var):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the value of theta for visualising the results of topic model\n",
    "        probabilities mass function over \"K\" topics for all documents (D) in the corpus\n",
    "    input:\n",
    "        the topic model\n",
    "        dataset\n",
    "        dependent variable\n",
    "    output:\n",
    "        D x K vector\n",
    "        D = number of documents\n",
    "        K = number of topics\n",
    "    \"\"\"\n",
    "    mat_theta = []\n",
    "    for line, dep in zip(data, dep_var):\n",
    "        pred_1 = []\n",
    "        pred_1.append(dep)\n",
    "        ch = \" \".join(line)\n",
    "        docu = ch.strip().split()\n",
    "        theta_val = model.infer(doc=model.make_doc(words=docu, y=pred_1),\n",
    "                                     iter=100,\n",
    "                                     workers=0,\n",
    "                                     together=False)\n",
    "        mat_theta.append(theta_val[0])\n",
    "        \n",
    "    with open(directo + '\\\\topic_probabilities_slda.csv', 'w') as f:\n",
    "        for item in mat_theta:\n",
    "            for items in item:\n",
    "                f.writelines(\"%s, \" %items)\n",
    "            f.writelines(\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return mat_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tokens per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_token(data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes number of tokens per document for the entire corpus\n",
    "    input:\n",
    "        dataset\n",
    "    output:\n",
    "        N x 1 vector\n",
    "        N = number of tokens in the document\n",
    "    \"\"\"\n",
    "    numb_tok = []\n",
    "    for text in data:\n",
    "        numb_tok.append(len(text))\n",
    "    return numb_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(vocabs, data):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function computes the frequency of words in the entire corpus\n",
    "    input:\n",
    "        list of words\n",
    "        dataset\n",
    "    output:\n",
    "        N x 1 vector\n",
    "        N = frequency of words in the document\n",
    "    \"\"\"\n",
    "    fre_words = []\n",
    "    for words in vocabs:\n",
    "        words_freq = 0\n",
    "        for line in data:\n",
    "            for ind_words in line:\n",
    "                if words == ind_words:\n",
    "                    words_freq += 1\n",
    "        fre_words.append(words_freq)\n",
    "    return fre_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Parameters for Visualising LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LDA model\n",
    "lda_model = tp.LDAModel.load('test.lda.bin')\n",
    "#lda_model.get_topic_word_dist(2)\n",
    "lvocab, lphi_val = compute_phi(lda_model)\n",
    "ltheta_val = compute_theta_lda(lda_model, data_lemmatized)\n",
    "lnum_token = num_token(data_lemmatized)\n",
    "lfreq_terms = freq_words(lvocab, data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting in pyLDAvis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic-Term shape: (3, 42)\n",
      "Doc-Topic shape: (145, 3)\n"
     ]
    }
   ],
   "source": [
    "# Visualising the Results\n",
    "pyLDAvis.enable_notebook()\n",
    "data_lda = {'topic_term_dists': lphi_val,\n",
    "            'doc_topic_dists' : ltheta_val,\n",
    "            'doc_lengths'     : lnum_token,\n",
    "            'vocab'           : lvocab,\n",
    "            'term_frequency'  : lfreq_terms}\n",
    "print('Topic-Term shape: %s' % str(np.array(data_lda['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(data_lda['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibabu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1024420482496901848371701506\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1024420482496901848371701506_data = {\"mdsDat\": {\"x\": [-0.26904871037193007, -0.12127338286931306, 0.39032209324124295], \"y\": [0.23988063222448952, -0.30917060964572907, 0.06928997742123971], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [41.93352940786191, 36.99254690944807, 21.07392368269001]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [115.0, 68.0, 31.0, 40.0, 34.0, 20.0, 28.0, 17.0, 74.0, 22.0, 18.0, 10.0, 10.0, 16.0, 28.0, 10.0, 15.0, 15.0, 8.0, 12.0, 10.0, 6.0, 11.0, 9.0, 5.0, 5.0, 5.0, 5.0, 33.0, 6.0, 115.51829195251901, 68.5544741755845, 22.8599545955233, 15.244199753371136, 15.244199753371136, 11.436322924348291, 7.628446687378682, 5.089862134696785, 5.089862134696785, 3.820569562329219, 19.05207658239398, 26.66783024043967, 1.2819851576606314, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 0.012692921792072143, 40.181977077607, 34.82616470206076, 28.131399754920185, 18.75872966459098, 16.08082452140234, 12.063965762034895, 10.725013190440576, 9.386059574261772, 6.708154431073132, 6.708154431073132, 4.030248243300008, 4.030248243300008, 22.775589468542908, 48.215692507172925, 14.74187194980802, 1.3523424472460654, 1.3523424472460654, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 0.013389528817053386, 31.038890982212383, 20.247447006461133, 17.549585417444163, 10.804933230139211, 10.804933230139211, 8.107072236201398, 6.758142036772071, 5.4092118373427445, 5.4092118373427445, 5.4092118373427445, 5.4092118373427445, 4.060280745294679, 4.060280745294679, 4.060280745294679, 9.456003030709883, 2.7113502483257728, 2.7113502483257728, 5.4092118373427445, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179, 0.01348930436531179], \"Term\": [\"book\", \"ticket\", \"travel\", \"googl\", \"find\", \"train\", \"rout\", \"check\", \"map\", \"hotel\", \"place\", \"time\", \"statu\", \"reserv\", \"locat\", \"know\", \"taxi\", \"cab\", \"transport\", \"gp\", \"search\", \"track\", \"flight\", \"traffic\", \"destin\", \"run\", \"public\", \"avail\", \"navig\", \"restaur\", \"book\", \"ticket\", \"hotel\", \"cab\", \"taxi\", \"flight\", \"bu\", \"uber\", \"servic\", \"fare\", \"navig\", \"map\", \"shortest\", \"updat\", \"reach\", \"schedul\", \"rail\", \"plan\", \"go\", \"destin\", \"public\", \"avail\", \"run\", \"restaur\", \"direct\", \"track\", \"transport\", \"traffic\", \"search\", \"know\", \"find\", \"googl\", \"gp\", \"locat\", \"check\", \"place\", \"travel\", \"train\", \"googl\", \"find\", \"rout\", \"place\", \"reserv\", \"gp\", \"search\", \"traffic\", \"direct\", \"restaur\", \"updat\", \"reach\", \"locat\", \"map\", \"navig\", \"schedul\", \"know\", \"fare\", \"shortest\", \"go\", \"rail\", \"plan\", \"uber\", \"servic\", \"destin\", \"public\", \"run\", \"avail\", \"track\", \"bu\", \"flight\", \"check\", \"cab\", \"hotel\", \"book\", \"travel\", \"train\", \"check\", \"statu\", \"time\", \"transport\", \"track\", \"public\", \"destin\", \"run\", \"avail\", \"rail\", \"go\", \"plan\", \"know\", \"shortest\", \"schedul\", \"locat\", \"fare\", \"updat\", \"reach\", \"uber\", \"servic\", \"restaur\", \"direct\", \"bu\", \"traffic\", \"search\", \"flight\", \"gp\", \"googl\", \"book\", \"cab\", \"ticket\", \"find\", \"taxi\"], \"Total\": [115.0, 68.0, 31.0, 40.0, 34.0, 20.0, 28.0, 17.0, 74.0, 22.0, 18.0, 10.0, 10.0, 16.0, 28.0, 10.0, 15.0, 15.0, 8.0, 12.0, 10.0, 6.0, 11.0, 9.0, 5.0, 5.0, 5.0, 5.0, 33.0, 6.0, 115.54517078570137, 68.58135300876687, 22.886833428705664, 15.271078586553502, 15.271078586553502, 11.463201757530657, 7.655325520561047, 5.11674096787915, 5.11674096787915, 3.8474483955115844, 33.80743783656732, 74.89701205197791, 4.006724934803458, 4.056430469457392, 4.056430469457392, 4.07638561736391, 4.086363195903805, 4.086363195903805, 4.086363195903805, 5.43529428795187, 5.43529428795187, 5.43529428795187, 5.43529428795187, 6.734336657230516, 6.734336657230516, 6.784224487381197, 8.133154686810524, 9.412241800419155, 10.751195416597959, 10.821038399748021, 34.85234692821815, 40.20815930376439, 12.090147988192278, 28.197494227677726, 17.575667868053287, 18.784911890748365, 31.064973432821507, 20.273529457070257, 40.20815930376439, 34.85234692821815, 28.15758198107757, 18.784911890748365, 16.107006747559726, 12.090147988192278, 10.751195416597959, 9.412241800419155, 6.734336657230516, 6.734336657230516, 4.056430469457392, 4.056430469457392, 28.197494227677726, 74.89701205197791, 33.80743783656732, 4.07638561736391, 10.821038399748021, 3.8474483955115844, 4.006724934803458, 4.086363195903805, 4.086363195903805, 4.086363195903805, 5.11674096787915, 5.11674096787915, 5.43529428795187, 5.43529428795187, 5.43529428795187, 5.43529428795187, 6.784224487381197, 7.655325520561047, 11.463201757530657, 17.575667868053287, 15.271078586553502, 22.886833428705664, 115.54517078570137, 31.064973432821507, 20.273529457070257, 17.575667868053287, 10.831015680748337, 10.831015680748337, 8.133154686810524, 6.784224487381197, 5.43529428795187, 5.43529428795187, 5.43529428795187, 5.43529428795187, 4.086363195903805, 4.086363195903805, 4.086363195903805, 10.821038399748021, 4.006724934803458, 4.07638561736391, 28.197494227677726, 3.8474483955115844, 4.056430469457392, 4.056430469457392, 5.11674096787915, 5.11674096787915, 6.734336657230516, 6.734336657230516, 7.655325520561047, 9.412241800419155, 10.751195416597959, 11.463201757530657, 12.090147988192278, 40.20815930376439, 115.54517078570137, 15.271078586553502, 68.58135300876687, 34.85234692821815, 15.271078586553502], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8689, 0.8687, 0.8679, 0.8673, 0.8673, 0.8667, 0.8656, 0.8638, 0.8638, 0.8621, 0.2956, -0.1636, -0.2705, -4.8979, -4.8979, -4.9028, -4.9053, -4.9053, -4.9053, -5.1905, -5.1905, -5.1905, -5.1905, -5.4048, -5.4048, -5.4122, -5.5936, -5.7396, -5.8726, -5.8791, -7.0487, -7.1917, -5.99, -6.8369, -6.3641, -6.4307, -6.9337, -6.5069, 0.9938, 0.9937, 0.9935, 0.9931, 0.9928, 0.9923, 0.992, 0.9917, 0.9906, 0.9906, 0.988, 0.988, 0.7809, 0.554, 0.1645, -0.1089, -1.0852, -4.6662, -4.7068, -4.7265, -4.7265, -4.7265, -4.9513, -4.9513, -5.0117, -5.0117, -5.0117, -5.0117, -5.2334, -5.3542, -5.758, -6.1853, -6.0448, -6.4494, -8.0685, 1.5563, 1.5558, 1.5556, 1.5547, 1.5547, 1.5539, 1.5533, 1.5523, 1.5523, 1.5523, 1.5523, 1.5507, 1.5507, 1.5507, 1.4223, 1.1666, 1.1494, -0.094, -4.0961, -4.149, -4.149, -4.3812, -4.3812, -4.6559, -4.6559, -4.7841, -4.9907, -5.1237, -5.1879, -5.2411, -6.4428, -7.4984, -5.4747, -6.9767, -6.2998, -5.4747], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.0122, -1.534, -2.6322, -3.0374, -3.0374, -3.3248, -3.7297, -4.1343, -4.1343, -4.4212, -2.8144, -2.4781, -5.5132, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -10.1283, -1.9428, -2.0859, -2.2993, -2.7046, -2.8586, -3.146, -3.2637, -3.397, -3.7329, -3.7329, -4.2424, -4.2424, -2.5105, -1.7605, -2.9455, -5.3344, -5.3344, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -9.9495, -1.6383, -2.0655, -2.2085, -2.6935, -2.6935, -2.9808, -3.1628, -3.3854, -3.3854, -3.3854, -3.3854, -3.6723, -3.6723, -3.6723, -2.8269, -4.0761, -4.0761, -3.3854, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794, -9.3794]}, \"token.table\": {\"Topic\": [3, 1, 1, 1, 3, 3, 2, 1, 2, 1, 3, 2, 2, 1, 2, 3, 2, 3, 1, 2, 1, 2, 2, 3, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 1, 1, 3, 3, 1, 1, 3, 3, 2, 3, 3, 3, 1, 2], \"Freq\": [0.9199133910896482, 1.0039363758018254, 1.0450241441089878, 0.9822488906060511, 1.0241431583216254, 0.9199133910896482, 1.0394490736491837, 1.0396500716335486, 1.0042365316770765, 0.9595922877980962, 0.9788655115163587, 0.9948229586390218, 0.9925436819896398, 1.004944614625123, 0.09241257290273418, 0.8317131561246076, 0.8156753154833156, 0.17732072075724253, 0.3604950219010369, 0.6408800389351766, 0.5620065055462123, 0.4436893464838519, 1.011450046212757, 0.9788655115163587, 0.9199133910896482, 0.9788655115163587, 0.9860886387965279, 0.9933565094224637, 1.0394490736491837, 0.99440356841779, 0.9199133910896482, 0.24531535871885282, 0.7359460761565585, 1.0231420389790264, 0.9771845069719176, 0.24958039702544568, 0.7487411910763371, 1.0156018903704502, 0.9822488906060511, 1.0061043851260796, 1.0156018903704502, 1.031805479464919, 0.9562015289066632, 0.9865080494420341, 0.9836281625103651, 0.9979084664932352, 0.9771845069719176, 0.9860886387965279], \"Term\": [\"avail\", \"book\", \"bu\", \"cab\", \"check\", \"destin\", \"direct\", \"fare\", \"find\", \"flight\", \"go\", \"googl\", \"gp\", \"hotel\", \"know\", \"know\", \"locat\", \"locat\", \"map\", \"map\", \"navig\", \"navig\", \"place\", \"plan\", \"public\", \"rail\", \"reach\", \"reserv\", \"restaur\", \"rout\", \"run\", \"schedul\", \"schedul\", \"search\", \"servic\", \"shortest\", \"shortest\", \"statu\", \"taxi\", \"ticket\", \"time\", \"track\", \"traffic\", \"train\", \"transport\", \"travel\", \"uber\", \"updat\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1024420482496901848371701506\", ldavis_el1024420482496901848371701506_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1024420482496901848371701506\", ldavis_el1024420482496901848371701506_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1024420482496901848371701506\", ldavis_el1024420482496901848371701506_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_lda = pyLDAvis.prepare(**data_lda)\n",
    "pyLDAvis.display(vis_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the Results of Supervised LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Parameters for Visualising Supervised LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sLDA model\n",
    "slda_model = tp.SLDAModel.load('test.slda.bin')\n",
    "#slda_model.get_topic_word_dist(2)\n",
    "svocab, sphi_val = compute_phi(slda_model)\n",
    "stheta_val = compute_theta_slda(slda_model, data_lemmatized, resp)\n",
    "snum_token = num_token(data_lemmatized)\n",
    "sfreq_terms = freq_words(svocab, data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting in pyLDAvis (sLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic-Term shape: (3, 42)\n",
      "Doc-Topic shape: (145, 3)\n"
     ]
    }
   ],
   "source": [
    "# Visualising the Results\n",
    "pyLDAvis.enable_notebook()\n",
    "data_slda = {'topic_term_dists': sphi_val,\n",
    "             'doc_topic_dists' : stheta_val,\n",
    "             'doc_lengths'     : snum_token,\n",
    "             'vocab'           : svocab,\n",
    "             'term_frequency'  : sfreq_terms}\n",
    "print('Topic-Term shape: %s' % str(np.array(data_slda['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(data_slda['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1024420482502382803642908714\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1024420482502382803642908714_data = {\"mdsDat\": {\"x\": [0.2759070785383602, -0.36019104723604656, 0.08428396869768612], \"y\": [-0.22878462797722549, -0.09863416463271761, 0.327418792609943], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [49.18819585342657, 32.49574300489027, 18.316061141683157]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [116.0, 72.0, 38.0, 68.0, 31.0, 34.0, 29.0, 17.0, 20.0, 18.0, 28.0, 22.0, 33.0, 11.0, 11.0, 11.0, 6.0, 6.0, 9.0, 15.0, 15.0, 15.0, 8.0, 8.0, 11.0, 11.0, 10.0, 3.0, 5.0, 5.0, 116.0411816733278, 68.86478036030657, 22.96342908340867, 15.313202481630801, 15.313202481630801, 15.313202481630801, 11.488089875222645, 11.488089875222645, 10.213051645272483, 6.3879390388643245, 5.112901156154551, 5.112901156154551, 3.8378632734447775, 30.613654296224986, 29.33861537179405, 5.112901156154551, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 0.012750376765357934, 31.945502694782192, 29.16884078266699, 20.838853211111264, 18.062189463785934, 11.120533765892871, 11.120533765892871, 11.120533765892871, 9.732202809835272, 8.343870936172607, 8.343870936172607, 5.567208565254876, 5.567208565254876, 5.567208565254876, 4.178877150394744, 4.178877150394744, 4.178877150394744, 4.178877150394744, 4.178877150394744, 4.178877150394744, 23.615515123226462, 4.178877150394744, 13.897196595613135, 2.7905459649358777, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 0.013883312643155512, 38.42889504758849, 17.940314751208554, 6.415487300087937, 6.415487300087937, 3.854414245836993, 43.551038052869664, 20.50138625384914, 1.2933417087895012, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486, 0.01280536248320486], \"Term\": [\"book\", \"map\", \"googl\", \"ticket\", \"travel\", \"find\", \"locat\", \"place\", \"train\", \"check\", \"rout\", \"hotel\", \"navig\", \"know\", \"statu\", \"time\", \"restaur\", \"direct\", \"traffic\", \"reserv\", \"taxi\", \"cab\", \"transport\", \"bu\", \"gp\", \"flight\", \"search\", \"reach\", \"public\", \"run\", \"book\", \"ticket\", \"hotel\", \"cab\", \"reserv\", \"taxi\", \"flight\", \"gp\", \"search\", \"track\", \"servic\", \"uber\", \"updat\", \"navig\", \"map\", \"rout\", \"reach\", \"schedul\", \"shortest\", \"go\", \"fare\", \"rail\", \"plan\", \"destin\", \"public\", \"avail\", \"run\", \"direct\", \"restaur\", \"bu\", \"locat\", \"find\", \"googl\", \"check\", \"know\", \"place\", \"travel\", \"locat\", \"train\", \"check\", \"time\", \"know\", \"statu\", \"traffic\", \"transport\", \"bu\", \"public\", \"run\", \"avail\", \"fare\", \"schedul\", \"shortest\", \"rail\", \"go\", \"plan\", \"rout\", \"destin\", \"find\", \"navig\", \"updat\", \"reach\", \"servic\", \"uber\", \"track\", \"direct\", \"restaur\", \"book\", \"cab\", \"ticket\", \"taxi\", \"flight\", \"googl\", \"place\", \"direct\", \"restaur\", \"reach\", \"map\", \"find\", \"destin\", \"updat\", \"go\", \"fare\", \"plan\", \"shortest\", \"schedul\", \"rail\", \"servic\", \"uber\", \"public\", \"avail\", \"run\", \"track\", \"bu\", \"transport\", \"traffic\", \"search\", \"know\", \"time\", \"statu\", \"gp\", \"flight\", \"navig\", \"locat\", \"hotel\", \"check\", \"cab\", \"book\", \"taxi\", \"travel\", \"train\"], \"Total\": [116.0, 72.0, 38.0, 68.0, 31.0, 34.0, 29.0, 17.0, 20.0, 18.0, 28.0, 22.0, 33.0, 11.0, 11.0, 11.0, 6.0, 6.0, 9.0, 15.0, 15.0, 15.0, 8.0, 8.0, 11.0, 11.0, 10.0, 3.0, 5.0, 5.0, 116.06787034845416, 68.89146903543293, 22.990117758535032, 15.339891156757162, 15.339891156757162, 15.339891156757162, 11.514778550349005, 11.514778550349005, 10.239740320398843, 6.414627713990685, 5.139589831280912, 5.139589831280912, 3.864551948571138, 33.41700562364407, 72.90353673730687, 28.74122164186422, 3.8810479352455065, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 5.4849692359496025, 5.592764304503438, 5.592764304503438, 5.592764304503438, 6.442120989496451, 6.442120989496451, 8.369426675421169, 29.194396521915554, 34.41133322622763, 38.455528736997, 18.087745203034498, 11.146089505141433, 17.966948440617067, 31.971058434030756, 29.194396521915554, 20.864408950359827, 18.087745203034498, 11.146089505141433, 11.146089505141433, 11.146089505141433, 9.757758549083833, 8.369426675421169, 8.369426675421169, 5.592764304503438, 5.592764304503438, 5.592764304503438, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 28.74122164186422, 5.4849692359496025, 34.41133322622763, 33.41700562364407, 3.864551948571138, 3.8810479352455065, 5.139589831280912, 5.139589831280912, 6.414627713990685, 6.442120989496451, 6.442120989496451, 116.06787034845416, 15.339891156757162, 68.89146903543293, 15.339891156757162, 11.514778550349005, 38.455528736997, 17.966948440617067, 6.442120989496451, 6.442120989496451, 3.8810479352455065, 72.90353673730687, 34.41133322622763, 5.4849692359496025, 3.864551948571138, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 4.204432889643306, 5.139589831280912, 5.139589831280912, 5.592764304503438, 5.592764304503438, 5.592764304503438, 6.414627713990685, 8.369426675421169, 8.369426675421169, 9.757758549083833, 10.239740320398843, 11.146089505141433, 11.146089505141433, 11.146089505141433, 11.514778550349005, 11.514778550349005, 33.41700562364407, 29.194396521915554, 22.990117758535032, 18.087745203034498, 15.339891156757162, 116.06787034845416, 15.339891156757162, 31.971058434030756, 20.864408950359827], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7093, 0.7091, 0.7084, 0.7078, 0.7078, 0.7078, 0.7072, 0.7072, 0.7069, 0.7053, 0.7043, 0.7043, 0.7026, 0.6219, -0.2007, -1.017, -5.0088, -5.0888, -5.0888, -5.0888, -5.0888, -5.0888, -5.0888, -5.3547, -5.3742, -5.3742, -5.3742, -5.5155, -5.5155, -5.7773, -7.0267, -7.1911, -7.3022, -6.5479, -6.0638, -6.5412, 1.1233, 1.1232, 1.1228, 1.1226, 1.1218, 1.1218, 1.1218, 1.1214, 1.121, 1.121, 1.1195, 1.1195, 1.1195, 1.118, 1.118, 1.118, 1.118, 1.118, 1.118, 0.9276, 0.8521, 0.2174, -1.3588, -4.5049, -4.5091, -4.79, -4.79, -5.0116, -5.0159, -5.0159, -7.9072, -5.8835, -7.3855, -5.8835, -5.5966, 1.6967, 1.6959, 1.6932, 1.6932, 1.6905, 1.1822, 1.1795, 0.2526, -4.0123, -4.0966, -4.0966, -4.0966, -4.0966, -4.0966, -4.0966, -4.2975, -4.2975, -4.382, -4.382, -4.382, -4.5191, -4.7851, -4.7851, -4.9386, -4.9868, -5.0716, -5.0716, -5.0716, -5.1041, -5.1041, -6.1696, -6.0345, -5.7956, -5.5557, -5.391, -7.4147, -5.391, -6.1253, -5.6985], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.1672, -1.689, -2.7873, -3.1925, -3.1925, -3.1925, -3.4799, -3.4799, -3.5975, -4.0668, -4.2894, -4.2894, -4.5763, -2.4997, -2.5423, -4.2894, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -10.2834, -2.0426, -2.1335, -2.4698, -2.6128, -3.0978, -3.0978, -3.0978, -3.2312, -3.3851, -3.3851, -3.7897, -3.7897, -3.7897, -4.0766, -4.0766, -4.0766, -4.0766, -4.0766, -4.0766, -2.3447, -4.0766, -2.8749, -4.4804, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -9.7837, -1.2845, -2.0462, -3.0746, -3.0746, -3.5841, -1.1594, -1.9128, -4.6761, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912, -9.2912]}, \"token.table\": {\"Topic\": [2, 1, 2, 1, 2, 2, 3, 3, 2, 2, 3, 1, 2, 3, 1, 1, 2, 2, 1, 3, 1, 2, 3, 2, 2, 2, 3, 1, 3, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1], \"Freq\": [1.0728147430008173, 0.9994152529183967, 0.9558599782579996, 0.9778426617709448, 0.9951489142483179, 0.7292657128837091, 0.18231642822092728, 0.9313702753771147, 0.9513768217951863, 0.4068427081264459, 0.6102640621896689, 0.9552940989617726, 0.9513768217951863, 0.9881544019297607, 0.9552940989617726, 1.000429847361756, 0.9868932054534422, 0.9933413070631679, 0.39778591407019975, 0.6035372489340962, 0.9276713883085345, 0.08977465048147107, 1.0018395755680032, 0.9513768217951863, 1.0728147430008173, 0.9513768217951863, 1.030649470642771, 0.9778426617709448, 0.9313702753771147, 0.17396616129625617, 0.8350375742220296, 1.0728147430008173, 0.9513768217951863, 0.9765872656046511, 0.9728402779476037, 0.9513768217951863, 0.9868932054534422, 0.9778426617709448, 1.0015753904813853, 0.9868932054534422, 0.9353621546755774, 1.0248255221419587, 1.006498676763994, 0.9558599782579996, 1.0009052426596687, 0.9728402779476037, 1.035048837027263], \"Term\": [\"avail\", \"book\", \"bu\", \"cab\", \"check\", \"destin\", \"destin\", \"direct\", \"fare\", \"find\", \"find\", \"flight\", \"go\", \"googl\", \"gp\", \"hotel\", \"know\", \"locat\", \"map\", \"map\", \"navig\", \"navig\", \"place\", \"plan\", \"public\", \"rail\", \"reach\", \"reserv\", \"restaur\", \"rout\", \"rout\", \"run\", \"schedul\", \"search\", \"servic\", \"shortest\", \"statu\", \"taxi\", \"ticket\", \"time\", \"track\", \"traffic\", \"train\", \"transport\", \"travel\", \"uber\", \"updat\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1024420482502382803642908714\", ldavis_el1024420482502382803642908714_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1024420482502382803642908714\", ldavis_el1024420482502382803642908714_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1024420482502382803642908714\", ldavis_el1024420482502382803642908714_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_slda = pyLDAvis.prepare(**data_slda)\n",
    "pyLDAvis.display(vis_slda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Scores for use in Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the code, values are computed for each document in the corpus. The values are computed based on the words used in each of the documents in the corpus. Scores will be computed for each topic. This will be based on the probability values in each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(list_dataset, list_word_prob):\n",
    "    \"\"\"\n",
    "    desc:\n",
    "        this function will take the cleaned dataset and list of word probabilities per topic and compute the scores\n",
    "    input:\n",
    "        cleaned dataset as a list\n",
    "        word probabilities as a dataframe\n",
    "    output:\n",
    "        scores for each document in the corpus\n",
    "    \"\"\"\n",
    "    n = len(list_dataset)\n",
    "    prob_list = [[0 for i in range(4)] for i in range(n)]\n",
    "    for index, document in enumerate(list_dataset):\n",
    "        # remember to change the number of variables based on the number of topics\n",
    "        probab_1 = 0\n",
    "        probab_2 = 0\n",
    "        probab_3 = 0\n",
    "        for word in document:\n",
    "            for index1, row in list_word_prob.iterrows():\n",
    "                item  = row['Word']\n",
    "                prob1 = row['Prob_1']\n",
    "                prob2 = row['Prob_2']\n",
    "                prob3 = row['Prob_3']\n",
    "                if word == item:\n",
    "                    probab_1 += prob1\n",
    "                    probab_2 += prob2\n",
    "                    probab_3 += prob3\n",
    "            \n",
    "            prob_list[index][0] = probab_1\n",
    "            prob_list[index][1] = probab_2\n",
    "            prob_list[index][2] = probab_3\n",
    "            prob_list[index][3] = probab_1 + probab_2 + probab_3\n",
    "    \n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Scores for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dist = pd.read_csv(directo + \"\\\\topic_word_prob_lda.csv\", header=None)\n",
    "lda_distT = lda_dist.T\n",
    "lda_distT.columns = ['Word', 'Prob_1', 'Prob_2', 'Prob_3']\n",
    "lda_distT['Word']   = lda_distT['Word'].str.strip()\n",
    "lda_distT['Prob_1'] = pd.to_numeric(lda_distT.Prob_1, errors='coerce')\n",
    "lda_distT['Prob_2'] = pd.to_numeric(lda_distT.Prob_2, errors='coerce')\n",
    "lda_distT['Prob_3'] = pd.to_numeric(lda_distT.Prob_3, errors='coerce')\n",
    "probab_lda = compute_scores(data_lemmatized, lda_distT)\n",
    "df['probab_lda'] = probab_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Scores for sLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "slda_dist = pd.read_csv(directo + \"\\\\topic_word_prob_slda.csv\", header=None)\n",
    "slda_distT = slda_dist.T\n",
    "slda_distT.columns = ['Word', 'Prob_1', 'Prob_2', 'Prob_3']\n",
    "slda_distT['Word']   = slda_distT['Word'].str.strip()\n",
    "slda_distT['Prob_1'] = pd.to_numeric(slda_distT.Prob_1, errors='coerce')\n",
    "slda_distT['Prob_2'] = pd.to_numeric(slda_distT.Prob_2, errors='coerce')\n",
    "slda_distT['Prob_3'] = pd.to_numeric(slda_distT.Prob_3, errors='coerce')\n",
    "probab_slda = compute_scores(data_lemmatized, slda_distT)\n",
    "df['probab_slda'] = probab_slda\n",
    "\n",
    "df.to_csv(directo + \"\\\\Open_Ended_Q1_Scores_Calibration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
